{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PW1 : LexicalAnalyzer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LexicalAnalyzer:\n",
    "    # ? constructor (like `public LexicalAnalyzer(String input_file, String output_file)` in java )\n",
    "    def __init__(self, input_file, output_file):\n",
    "        self.input_file = input_file\n",
    "        self.output_file = output_file\n",
    "        self.tokens = []\n",
    "    \n",
    "    def read_file(self,r_file):\n",
    "        with open(r_file, 'r') as file:\n",
    "            return file.read()\n",
    "\n",
    "    def analyze(self, text):\n",
    "        i = 0\n",
    "        while i < len(text):\n",
    "            char = text[i]\n",
    "\n",
    "            if char in ['>', '<', '=']:  \n",
    "                token = char\n",
    "                next_char = text[i+1] if i+1 < len(text) else ''\n",
    "                \n",
    "                if next_char == '=':\n",
    "                    token += '='\n",
    "                    i += 2\n",
    "                else:\n",
    "                    i += 1\n",
    "                \n",
    "                self.tokens.append(('cs', token))\n",
    "            elif char.isspace():  \n",
    "                i += 1\n",
    "            else:\n",
    "                raise ValueError(f\"Unrecognized character: {char}\")\n",
    "            \n",
    "    def make_tokens_file(self):\n",
    "        with open(self.output_file, 'w') as file:\n",
    "            for token in self.tokens:\n",
    "                file.write(f\"{token}\\n\")\n",
    "\n",
    "    def run(self):\n",
    "        try:\n",
    "            text = self.read_file(self.input_file)\n",
    "            self.analyze(text)\n",
    "            self.make_tokens_file()\n",
    "            print(\"Tokens file:\")\n",
    "            print(self.read_file(self.output_file))\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "analyzer = LexicalAnalyzer(\"test.txt\", \"token.txt\")\n",
    "analyzer.run()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
